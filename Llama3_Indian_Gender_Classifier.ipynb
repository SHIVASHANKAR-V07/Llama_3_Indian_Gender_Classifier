{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKWUgHR9sVRNX1LXldx1eM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHIVASHANKAR-V07/Llama_3_Indian_Gender_Classifier/blob/main/Llama3_Indian_Gender_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1] IMPORTING DEPENDENCIES :-**"
      ],
      "metadata": {
        "id": "1q_jGGGTQ5Nf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEr-3RFCQw9R",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install unsloth_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2] LOADING DATASET :-**"
      ],
      "metadata": {
        "id": "T3GK-DCWWFkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) Connecting Drive :**"
      ],
      "metadata": {
        "id": "83ET3wLIbDt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xe5JpEl8WaHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) JSON Format of Dataset :**"
      ],
      "metadata": {
        "id": "t-fueeShaSC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Setting file path\n",
        "input_csv = \"/content/drive/MyDrive/Train_Labels_Dataset.csv\"\n",
        "output_file = \"/content/drive/MyDrive/train_dataset.jsonl\"\n",
        "\n",
        "# Mapping Labels\n",
        "label_map = {\n",
        "    0: \"neutral\",\n",
        "    1: \"male\",\n",
        "    2: \"female\"\n",
        "}\n",
        "\n",
        "# Prompt Template\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the gender of the given Indian name.\n",
        "\n",
        "### Input:\n",
        "{name}\n",
        "\n",
        "### Response:\n",
        "{gender}\"\"\"\n",
        "\n",
        "# Loading CSV\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "# JSONL File Creation\n",
        "with open(output_file, \"w\") as f:\n",
        "    for _, row in df.iterrows():\n",
        "        # Lowercasing Names\n",
        "        name_val = str(row['Name']).strip().lower()\n",
        "\n",
        "        # Labels Mapping\n",
        "        gender_val = label_map.get(row['Label'], \"neutral\")\n",
        "\n",
        "        # Prompt Formatting\n",
        "        full_text = prompt_template.format(name=name_val, gender=gender_val)\n",
        "\n",
        "        # EOS token for Llama 3\n",
        "        full_text += \" <|end_of_text|>\"\n",
        "\n",
        "        # Saving\n",
        "        json.dump({\"text\": full_text}, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Verifying\n",
        "print(f\"Successfully converted {len(df)} rows!\")\n",
        "print(f\"File saved at: {output_file}\")\n",
        "!head -n 5 {output_file}"
      ],
      "metadata": {
        "id": "JWZ5_Fmqbty3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3] MODEL INTEGRATION :-**\n",
        "- ## unsloth/llama-3-8b-bnb-4bit ü¶•ü¶ô"
      ],
      "metadata": {
        "id": "jWxNJqoAbU-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) Checkpoints Storage :**"
      ],
      "metadata": {
        "id": "broyKYfEkGM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory to store checkpoints\n",
        "DRIVE_OUTPUT_DIR=\"/content/drive/MyDrive/Llama3_FineTune_Checkpoints\""
      ],
      "metadata": {
        "id": "WUTUE0dCkDSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) Resume Training - Error Blocker :**"
      ],
      "metadata": {
        "id": "gHgx8UjXmpaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this only when resuming the training to tackle \"UnpicklingError\"\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. Store the original torch.load function reference\n",
        "original_load = torch.load\n",
        "\n",
        "# 2. Define the patched function\n",
        "def patched_load(*args, **kwargs):\n",
        "    \"\"\"Overrides torch.load to set weights_only=False for safe checkpoint loading.\"\"\"\n",
        "    # Force the weights_only argument to False to bypass the NumPy global block\n",
        "    kwargs['weights_only'] = False\n",
        "\n",
        "    # Optional: You can also try allow-listing the NumPy global if the above doesn't work:\n",
        "    # from torch.serialization import add_safe_globals\n",
        "    # add_safe_globals([np.core.multiarray._reconstruct])\n",
        "\n",
        "    return original_load(*args, **kwargs)\n",
        "\n",
        "# 3. Replace the official torch.load with your patched version\n",
        "torch.load = patched_load\n",
        "\n",
        "print(\"Successfully patched torch.load to allow checkpoint resumption.\")\n",
        "\n",
        "import os\n",
        "# Set this to prevent Unsloth from using its custom compiled cache\n",
        "os.environ['UNSLOTH_ALWAYS_RESTART_TRAINER'] = 'True'"
      ],
      "metadata": {
        "id": "mZQRMQnKnYpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **c) Unsloth Latest Version :**"
      ],
      "metadata": {
        "id": "q_sfyoS6pcik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo"
      ],
      "metadata": {
        "id": "9608Hk2uptIh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **d) Fine-Tuning Model :**"
      ],
      "metadata": {
        "id": "8v2gwft7nl18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Model Integration Libraries\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/llama-3-8b-bnb-4bit\" # Base-Model\n",
        "\n",
        "# Loading Model and Tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Customizing Model Architecture\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                     \"gate_proj\", \"up_proj\", \"down_proj\",], # Relation Processing ( adaptation )\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(\"json\", data_files = \"/content/drive/MyDrive/train_dataset.jsonl\", split = \"train\")\n",
        "\n",
        "# Training the model\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\", # Tokenization\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        learning_rate = 2e-4,\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        output_dir = DRIVE_OUTPUT_DIR, # Output Folder\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 500,\n",
        "        save_total_limit = 2,\n",
        "        seed = 3407,\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Training Session has started !\")\n",
        "\n",
        "trainer.train(resume_from_checkpoint = True)  # 'False' for First-Time\n",
        "\n",
        "print(\"Training Completed...\")"
      ],
      "metadata": {
        "id": "CyyAPlFClFkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4] MODEL INFERENCE TESTING :-**"
      ],
      "metadata": {
        "id": "Ta_KzWxE5FuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) Single-Name Testing :**"
      ],
      "metadata": {
        "id": "IMMmRW26490Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test Prompt\n",
        "# Should be the same prompt format used during training !!!\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the gender of the given Indian name.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "test_name = \"pragathish\"\n",
        "\n",
        "# Run Inference\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        prompt.format(test_name) # Test Name\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\n",
        "# Ensuring the test prompt isn't generated in the output\n",
        "input_len = inputs.input_ids.shape[1]\n",
        "final_output = outputs[:, input_len:]\n",
        "\n",
        "result = tokenizer.batch_decode(final_output, skip_special_tokens = True)[0]\n",
        "\n",
        "print(f\"Name: {test_name.ljust(10)} | Predicted: {result.strip()}\")"
      ],
      "metadata": {
        "id": "F-GGwG380Zcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) Multi-Name Testing :**"
      ],
      "metadata": {
        "id": "D5lIY-K15X5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_names = [\"ragul\", \"ananya\", \"kisan\", \"preeti\"]\n",
        "\n",
        "for name in test_names:\n",
        "    inputs = tokenizer([prompt.format(name)], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 10)\n",
        "\n",
        "    # Ensuring the test prompt isn't generated in the out\n",
        "    prediction = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Name: {name.ljust(10)} | Predicted: {prediction.strip()}\")"
      ],
      "metadata": {
        "id": "HDmOBF265by4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5] SAVING MODEL :-**\n",
        ">  Save the model as soon as the **\"Training\"** is finished\n",
        "\n",
        "> Use any one of the below **\"Saving Mechanism\"** according to the use-case and storage capacity available\n",
        "\n",
        "\n",
        "> **\"Mount your Drive\"** and run these below code blocks\n"
      ],
      "metadata": {
        "id": "S23QOE0c7Oga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) LoRA Adapters :**\n",
        "- ***Developers ( ~200 MB )***"
      ],
      "metadata": {
        "id": "gmbEiLd6BlPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Fine_Tuned_Model/LoRA\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "print(f\"‚úÖ Model successfully saved to {model_path}\")"
      ],
      "metadata": {
        "id": "V-N7c5UU8C2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) GGUF :**\n",
        "- ***Laptop Users ( ~5 GB ) - runs on CPU***"
      ],
      "metadata": {
        "id": "qPGXtCtlCl4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Fine_Tuned_Model/GGUF\"\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    model_path,\n",
        "    tokenizer,\n",
        "    quantization_method = \"q4_k_m\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ GGUF format successfully saved to: {model_path}\")"
      ],
      "metadata": {
        "id": "bjOjGIwHDPjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **c) MERGED :**\n",
        "- ***Production API's ( ~16.0 GB ) - faster deployment***"
      ],
      "metadata": {
        "id": "0Pgn3KPmDp9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Fine_Tuned_Model/Merged\"\n",
        "\n",
        "model.save_pretrained_merged(\n",
        "    model_path,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\" # Use 'merged_4bit' for saving space (~5.5GB)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Merged model successfully saved to: {model_path}_merged\")"
      ],
      "metadata": {
        "id": "-9-0eUBEEKAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6] PUBLISHING MODEL :-**\n",
        "- ## huggingface ü§ó\n",
        "\n",
        "- ## Two ways to publish -\n",
        "\n",
        "\n",
        "      1.   Using the \"Inference Model\"\n",
        "      2.   Using the \"Drive Folders\"\n",
        "\n"
      ],
      "metadata": {
        "id": "PDsEH86iuwve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) Logging into Hugging Face :**\n",
        "- ***Using \"write\" token - Saved in 'Secrets'***\n",
        "- ***\"Llama-3-Indian-Gender-Classifier\"*** **- Public Model Repository**"
      ],
      "metadata": {
        "id": "1f4vedRTx66s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "api = HfApi()\n",
        "hf_token = userdata.get('HF_TOKEN') # Accessing Secrets\n",
        "login(hf_token) # Using \"write\" token\n",
        "\n",
        "print(\"‚úÖ Successfully logged in via Colab Secrets!\")\n",
        "\n",
        "# Model Repository\n",
        "repo_id = \"shisha-07/Llama-3-Indian-Gender-Classifier\""
      ],
      "metadata": {
        "id": "IRayls9Zy1Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) Connecting to Drive :**"
      ],
      "metadata": {
        "id": "HjItiWIVCrwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting with Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fv9LB5HfCdec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **c) Using the \"Inference Model\" :**"
      ],
      "metadata": {
        "id": "kC0_DPUdE_iY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **i) Loading Inference Model -**"
      ],
      "metadata": {
        "id": "Nt2Ty1gc0cCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Specifying the File Path\n",
        "model_path = '/content/drive/MyDrive/Fine_Tuned_Model/LoRA'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path, # Loading the Inference Model\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Successfully Loaded Inference Modal\")"
      ],
      "metadata": {
        "id": "upgr20n70hF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ii) LoRA Adapter Model -**\n"
      ],
      "metadata": {
        "id": "3X0OGIRtzIP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\n",
        "    repo_id,\n",
        "    commit_message=\"Initial upload of LoRA Adapters\"\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    repo_id,\n",
        "    commit_message=\"Uploaded Tokenizer\"\n",
        ")\n",
        "\n",
        "print(\"LoRA Success!\")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "rIWuuKBv0O9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **iii) GGUF Model -**"
      ],
      "metadata": {
        "id": "UxWZwml236jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\n",
        "    repo_id,\n",
        "    tokenizer,\n",
        "    quantization_method = \"q4_k_m\",\n",
        "    commit_message=\"Added GGUF q4_k_m Version for Local Inference (Ollama/LM Studio)\"\n",
        ")\n",
        "\n",
        "print(\"GGUF Success\")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "_KKr2Bg74IxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **iv) Merged Model -**"
      ],
      "metadata": {
        "id": "Hfx7C9Zd39RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\n",
        "    repo_id,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        "    commit_message=\"Added Standalone 16-bit Merged Model for Production Deployment\"\n",
        ")\n",
        "\n",
        "print(\"Merge Success\")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "iEQDMZA74Tfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **d) Using the \"Drive Folders\" :**"
      ],
      "metadata": {
        "id": "MYYjOBu2Flqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **i) LoRA Adapter Model -**"
      ],
      "metadata": {
        "id": "w3h_mVF8FtkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Uploading LoRA...\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/Fine_Tuned_Model/LoRA\",\n",
        "    path_in_repo=\"LoRA\", # Folder in HF\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Uploading LoRA adapters\"\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "B0DQ3rk5FsPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ii) GGUF Model -**"
      ],
      "metadata": {
        "id": "wZPuRKukF0H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Uploading GGUF...\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/Fine_Tuned_Model/GGUF_gguf\",\n",
        "    path_in_repo=\"GGUF\", # Folder in HF\n",
        "    repo_id=repo_id,\n",
        "    commit_message=\"Uploaded GGUF q4_k_m\"\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "zvr-P6eVF0kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **iii) Merged Model -**"
      ],
      "metadata": {
        "id": "P_kLmDmBF06b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Uploading Merged...\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/Fine_Tuned_Model/Merged\",\n",
        "    path_in_repo=\"Merged\", # Folder in HF\n",
        "    repo_id=repo_id,\n",
        "    ignore_patterns=[\"**/.cache/*\", \"**/README.md\"],\n",
        "    commit_message=\"Uploaded Merged Model Weights\"\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Success! View your model at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "IxyoRqiTF1cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7] USING THE MODEL :-**"
      ],
      "metadata": {
        "id": "NRGzd9TYQPAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) LoRA Adapters :**"
      ],
      "metadata": {
        "id": "gPYRvhONVmal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import snapshot_download\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Variables\n",
        "MODEL_ID = \"shisha-07/Llama-3-Indian-Gender-Classifier\"\n",
        "NAME_TO_TEST = \"Aarav\"\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading adapter files from '{MODEL_ID}/LoRA'...\")\n",
        "\n",
        "download_path = snapshot_download(\n",
        "    repo_id=MODEL_ID,\n",
        "    allow_patterns=[\"LoRA/*\"],\n",
        "    local_dir=\"downloaded_adapters\",\n",
        ")\n",
        "\n",
        "local_adapter_path = os.path.join(download_path, \"LoRA\")\n",
        "\n",
        "print(f\"‚úÖ Adapters downloaded to: {local_adapter_path}\")\n",
        "\n",
        "# Loading Base Model\n",
        "print(f\"üîÑ Loading Base Llama-3 Model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", # Official base model (Safe from config errors)\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Loading Adapters\n",
        "print(f\"üîó Attaching Adapters...\")\n",
        "model.load_adapter(local_adapter_path) # Now we load from the folder we just downloaded\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Run Inference\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the gender of the given Indian name.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(f\"üöÄ Testing Name: {NAME_TO_TEST}\")\n",
        "inputs = tokenizer(\n",
        "    [alpaca_prompt.format(NAME_TO_TEST)],\n",
        "    return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 10,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "prediction = decoded.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"Name:       {NAME_TO_TEST}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "geSxpXHSnIew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) MERGED :**"
      ],
      "metadata": {
        "id": "yejBIYvKVslr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id=\"shisha-07/Llama-3-Indian-Gender-Classifier\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder=\"Merged\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    subfolder=\"Merged\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "prompt = \"Name: aarav\\nGender:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "output = model.generate(**inputs, max_new_tokens=5)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "tGQ--pCjVt3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **c) GGUF :**"
      ],
      "metadata": {
        "id": "H3UTpGQ6UGZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Download the .gguf file from HF 'GGUF' folder first\n",
        "model = Llama(\n",
        "    model_path=\"/content/drive/MyDrive/Fine_Tuned_Model/GGUF_gguf/llama-3-8b.Q4_K_M.gguf\", # Path of GGUF file\n",
        "    n_ctx=2048,\n",
        ")\n",
        "\n",
        "output = model(\n",
        "    \"Name: priya\\nGender:\",\n",
        "    max_tokens=10,\n",
        "    stop=[\"\\n\"]\n",
        ")\n",
        "\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dP7zSPUIUOce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8] EVALUATING THE MODEL :-**"
      ],
      "metadata": {
        "id": "CevnOygnXbt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- PATH ---\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Fine_Tuned_Model/Merged\"\n",
        "\n",
        "# --- TEST DATA ---\n",
        "test_data = [\n",
        "    # Male Names\n",
        "    {\"name\": \"Arjun\", \"label\": \"male\"}, {\"name\": \"Vikram\", \"label\": \"male\"},\n",
        "    {\"name\": \"Rohan\", \"label\": \"male\"}, {\"name\": \"Ishaan\", \"label\": \"male\"},\n",
        "    {\"name\": \"Mohammed\", \"label\": \"male\"}, {\"name\": \"Siddharth\", \"label\": \"male\"},\n",
        "    {\"name\": \"Aditya\", \"label\": \"male\"}, {\"name\": \"Varun\", \"label\": \"male\"},\n",
        "    {\"name\": \"Karan\", \"label\": \"male\"}, {\"name\": \"Abhishek\", \"label\": \"male\"},\n",
        "    {\"name\": \"Sanjay\", \"label\": \"male\"}, {\"name\": \"Rajesh\", \"label\": \"male\"},\n",
        "    {\"name\": \"Aakash\", \"label\": \"male\"}, {\"name\": \"Kartik\", \"label\": \"male\"},\n",
        "    {\"name\": \"Harish\", \"label\": \"male\"}, {\"name\": \"Ganesh\", \"label\": \"male\"},\n",
        "    {\"name\": \"Manish\", \"label\": \"male\"}, {\"name\": \"Prateek\", \"label\": \"male\"},\n",
        "    {\"name\": \"Vivek\", \"label\": \"male\"}, {\"name\": \"Ashok\", \"label\": \"male\"},\n",
        "    {\"name\": \"Vijay\", \"label\": \"male\"}, {\"name\": \"Nitin\", \"label\": \"male\"},\n",
        "    {\"name\": \"Rahul\", \"label\": \"male\"}, {\"name\": \"Manoj\", \"label\": \"male\"},\n",
        "    {\"name\": \"Dilip\", \"label\": \"male\"},\n",
        "\n",
        "    # Female Names\n",
        "    {\"name\": \"Deepika\", \"label\": \"female\"}, {\"name\": \"Saritha\", \"label\": \"female\"},\n",
        "    {\"name\": \"Ananya\", \"label\": \"female\"}, {\"name\": \"Kavita\", \"label\": \"female\"},\n",
        "    {\"name\": \"Fatima\", \"label\": \"female\"}, {\"name\": \"Priyanka\", \"label\": \"female\"},\n",
        "    {\"name\": \"Meenakshi\", \"label\": \"female\"}, {\"name\": \"Shweta\", \"label\": \"female\"},\n",
        "    {\"name\": \"Tanvi\", \"label\": \"female\"}, {\"name\": \"Riya\", \"label\": \"female\"},\n",
        "    {\"name\": \"Nandini\", \"label\": \"female\"}, {\"name\": \"Pooja\", \"label\": \"female\"},\n",
        "    {\"name\": \"Sneha\", \"label\": \"female\"}, {\"name\": \"Ishani\", \"label\": \"female\"},\n",
        "    {\"name\": \"Amrita\", \"label\": \"female\"}, {\"name\": \"Divya\", \"label\": \"female\"},\n",
        "    {\"name\": \"Jyoti\", \"label\": \"female\"}, {\"name\": \"Rashmi\", \"label\": \"female\"},\n",
        "    {\"name\": \"Simran\", \"label\": \"female\"}, {\"name\": \"Lata\", \"label\": \"female\"},\n",
        "    {\"name\": \"Bhavna\", \"label\": \"female\"}, {\"name\": \"Sonal\", \"label\": \"female\"},\n",
        "    {\"name\": \"Preeti\", \"label\": \"female\"}, {\"name\": \"Geeta\", \"label\": \"female\"},\n",
        "\n",
        "    # Neutal Names ---\n",
        "    {\"name\": \"Kiran\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Sonu\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Suman\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Krishna\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Happy\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Lucky\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Deepu\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Gurpreet\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Sukhdeep\", \"label\": \"neutral\"},\n",
        "    {\"name\": \"Raj\", \"label\": \"male\"} # Control Check\n",
        "]\n",
        "\n",
        "# --- LOAD MODEL ---\n",
        "print(f\"üîÑ Loading model from: {MODEL_PATH}...\")\n",
        "\n",
        "# 4-bit quantization configuration (Efficient for T4 GPU)\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure tokenizer has a padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- INFERENCE LOOP ---\n",
        "print(f\"\\nüöÄ Evaluating {len(test_data)} names...\")\n",
        "\n",
        "y_true = [item['label'] for item in test_data]\n",
        "y_pred = []\n",
        "\n",
        "# Alpaca Prompt Template (Must match training!)\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the gender of the given Indian name.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "for item in tqdm(test_data):\n",
        "    # Format input using template\n",
        "    inputs = tokenizer(\n",
        "        [alpaca_prompt.format(item['name'])],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Extract prediction\n",
        "    try:\n",
        "        # Getting text after '### Response:' and remove whitespace\n",
        "        prediction = decoded.split(\"### Response:\")[-1].strip().lower()\n",
        "        prediction = prediction.split()[0] if prediction else \"unknown\"\n",
        "    except:\n",
        "        prediction = \"error\"\n",
        "\n",
        "    y_pred.append(prediction)\n",
        "\n",
        "# --- RESULTS ---\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\n‚ú® FINAL EVALUATION RESULTS ‚ú®\")\n",
        "print(f\"‚úÖ Accuracy: {accuracy:.1%}\")\n",
        "print(f\"‚öñÔ∏è F1 Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\nüìã Detailed Classification Report:\")\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=[\"male\", \"female\", \"neutral\"],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Mismatch Analysis\n",
        "print(\"\\n‚ùå Mismatches:\")\n",
        "mismatches = [(data['name'], data['label'], pred) for data, pred in zip(test_data, y_pred) if data['label'] != pred]\n",
        "\n",
        "if not mismatches:\n",
        "    print(\"None! Perfect score on this set.\")\n",
        "else:\n",
        "    print(f\"{'Name':<15} | {'True Label':<10} | {'Predicted':<10}\")\n",
        "    print(\"-\" * 40)\n",
        "    for name, true, pred in mismatches:\n",
        "        print(f\"{name:<15} | {true:<10} | {pred:<10}\")"
      ],
      "metadata": {
        "id": "VuMgYZf6Xh2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}